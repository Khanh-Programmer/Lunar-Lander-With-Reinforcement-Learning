# ğŸš€ LunarLander-v3 vá»›i DQN & Double DQN

## ğŸ“‘ Má»¥c lá»¥c
- [ğŸš€ LunarLander-v3 vá»›i DQN \& Double DQN](#-lunarlander-v3-vá»›i-dqn--double-dqn)
  - [ğŸ“‘ Má»¥c lá»¥c](#-má»¥c-lá»¥c)
  - [ğŸ” Tá»•ng quan dá»± Ã¡n](#-tá»•ng-quan-dá»±-Ã¡n)
  - [ğŸŒŒ MÃ´i trÆ°á»ng](#-mÃ´i-trÆ°á»ng)
  - [ğŸ§  Thuáº­t toÃ¡n](#-thuáº­t-toÃ¡n)
    - [âœ… DQN (Deep Q-Network)](#-dqn-deep-q-network)
    - [âœ… Double DQN](#-double-dqn)
  - [ğŸ§ª Kiá»ƒm tra phiÃªn báº£n thÆ° viá»‡n](#-kiá»ƒm-tra-phiÃªn-báº£n-thÆ°-viá»‡n)
  - [ğŸ“Š Káº¿t quáº£ huáº¥n luyá»‡n](#-káº¿t-quáº£-huáº¥n-luyá»‡n)
  - [ğŸ“½ï¸ Video mÃ´ phá»ng mÃ´ hÃ¬nh (Double DQN)](#ï¸-video-mÃ´-phá»ng-mÃ´-hÃ¬nh-double-dqn)
  - [ğŸ”® Cáº£i tiáº¿n trong tÆ°Æ¡ng lai](#-cáº£i-tiáº¿n-trong-tÆ°Æ¡ng-lai)
    - [ğŸš€ 1. Sá»­ dá»¥ng thuáº­t toÃ¡n nÃ¢ng cao hÆ¡n](#-1-sá»­-dá»¥ng-thuáº­t-toÃ¡n-nÃ¢ng-cao-hÆ¡n)
    - [ğŸ§  2. Cáº£i thiá»‡n kiáº¿n trÃºc máº¡ng há»c sÃ¢u](#-2-cáº£i-thiá»‡n-kiáº¿n-trÃºc-máº¡ng-há»c-sÃ¢u)
    - [ğŸ§® 3. Há»— trá»£ huáº¥n luyá»‡n song song hoáº·c phÃ¢n tÃ¡n](#-3-há»—-trá»£-huáº¥n-luyá»‡n-song-song-hoáº·c-phÃ¢n-tÃ¡n)
    - [ğŸ¯ 4. Má»Ÿ rá»™ng sang mÃ´i trÆ°á»ng phá»©c táº¡p hÆ¡n](#-4-má»Ÿ-rá»™ng-sang-mÃ´i-trÆ°á»ng-phá»©c-táº¡p-hÆ¡n)
    - [ğŸ›  5. Cáº£i thiá»‡n pháº§n hiá»ƒn thá»‹ vÃ  phÃ¢n tÃ­ch](#-5-cáº£i-thiá»‡n-pháº§n-hiá»ƒn-thá»‹-vÃ -phÃ¢n-tÃ­ch)
  - [ğŸ“š TÃ i liá»‡u tham kháº£o](#-tÃ i-liá»‡u-tham-kháº£o)
    - [ğŸ“„ BÃ i bÃ¡o khoa há»c \& lÃ½ thuyáº¿t](#-bÃ i-bÃ¡o-khoa-há»c--lÃ½-thuyáº¿t)
    - [ğŸ“š TÃ i liá»‡u ká»¹ thuáº­t \& thÆ° viá»‡n](#-tÃ i-liá»‡u-ká»¹-thuáº­t--thÆ°-viá»‡n)
    - [ğŸ¥ Há»c thÃ´ng qua thá»±c hÃ nh \& nguá»“n má»Ÿ](#-há»c-thÃ´ng-qua-thá»±c-hÃ nh--nguá»“n-má»Ÿ)

---

## ğŸ” Tá»•ng quan dá»± Ã¡n

Dá»± Ã¡n nÃ y triá»ƒn khai vÃ  so sÃ¡nh hai thuáº­t toÃ¡n há»c tÄƒng cÆ°á»ng phá»• biáº¿n: **DQN (Deep Q-Network)** vÃ  **Double DQN** trÃªn mÃ´i trÆ°á»ng **LunarLander-v3** tá»« OpenAI Gym. Má»¥c tiÃªu lÃ  giÃºp agent Ä‘iá»u khiá»ƒn tÃ u Ä‘á»• bá»™ má»™t cÃ¡ch tá»‘i Æ°u vÃ  an toÃ n, Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»ƒm sá»‘ cao nháº¥t cÃ³ thá»ƒ.

---

## ğŸŒŒ MÃ´i trÆ°á»ng

ChÃºng tÃ´i sá»­ dá»¥ng **LunarLander-v3**, má»™t mÃ´i trÆ°á»ng mÃ´ phá»ng viá»‡c Ä‘á»• bá»™ tÃ u vÅ© trá»¥:

- Quan sÃ¡t: vector 8 chiá»u (vá»‹ trÃ­, váº­n tá»‘c, gÃ³c, tiáº¿p xÃºc máº·t Ä‘áº¥t)
- HÃ nh Ä‘á»™ng: 4 hÃ nh Ä‘á»™ng rá»i ráº¡c (khÃ´ng lÃ m gÃ¬, Ä‘áº©y Ä‘á»™ng cÆ¡ chÃ­nh, trÃ¡i, pháº£i)
- Pháº§n thÆ°á»Ÿng:
  - ThÆ°á»Ÿng cho háº¡ cÃ¡nh má»m
  - Pháº¡t náº¿u rÆ¡i máº¡nh hoáº·c rÆ¡i ngoÃ i khu vá»±c Ä‘Ã­ch
- Káº¿t thÃºc khi:
  - Agent háº¡ cÃ¡nh thÃ nh cÃ´ng
  - Ra khá»i khung hÃ¬nh hoáº·c háº¿t bÆ°á»›c

---

## ğŸ§  Thuáº­t toÃ¡n

### âœ… DQN (Deep Q-Network)

- Sá»­ dá»¥ng máº¡ng neural Ä‘á»ƒ Æ°á»›c lÆ°á»£ng hÃ m Q.
- Cáº­p nháº­t giÃ¡ trá»‹ Q thÃ´ng qua Bellman Equation.
- Gáº·p váº¥n Ä‘á» **overestimation bias** vÃ¬ sá»­ dá»¥ng cÃ¹ng má»™t máº¡ng Ä‘á»ƒ chá»n vÃ  Ä‘Ã¡nh giÃ¡ hÃ nh Ä‘á»™ng.

### âœ… Double DQN

- Giáº£m thiá»ƒu overestimation báº±ng cÃ¡ch:
  - DÃ¹ng **máº¡ng chÃ­nh** Ä‘á»ƒ chá»n hÃ nh Ä‘á»™ng tá»‘i Æ°u.
  - DÃ¹ng **máº¡ng má»¥c tiÃªu** Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hÃ nh Ä‘á»™ng Ä‘Ã³.
- ThÆ°á»ng á»•n Ä‘á»‹nh vÃ  chÃ­nh xÃ¡c hÆ¡n DQN cÆ¡ báº£n.

## ğŸ§ª Kiá»ƒm tra phiÃªn báº£n thÆ° viá»‡n

Äá»ƒ Ä‘áº£m báº£o dá»± Ã¡n hoáº¡t Ä‘á»™ng Ä‘Ãºng cÃ¡ch ( táº¥t cáº£ cÃ³ trong requirement.txt ), sá»­ dá»¥ng chÃ­nh xÃ¡c cÃ¡c phiÃªn báº£n thÆ° viá»‡n Ä‘Ã£ Ä‘Æ°á»£c kiá»ƒm chá»©ng bÃªn dÆ°á»›i:

| ğŸ“¦ ThÆ° viá»‡n        | âœ… PhiÃªn báº£n tÆ°Æ¡ng thÃ­ch |
|--------------------|--------------------------|
| `gymnasium`        | `1.1.1`                  |
| `torch`            | `2.5.1+cu121`            |
| `matplotlib`       | `3.10.1`                 |
| `ipython`          | `9.1.0`                  |

---

## ğŸ“Š Káº¿t quáº£ huáº¥n luyá»‡n
Vá»›i thuáº­t toÃ¡n DQN, tá»· lá»‡ háº¡ cÃ¡nh thÃ nh cÃ´ng cá»§a tÃ u vÅ© Ä‘áº¡t má»©c khÃ¡ cao vá»›i 95.6%, Double-DQN phiÃªn báº£n cáº£i tiáº¿n cá»§a thuáº­t toÃ¡n DQN Ä‘áº¡t tá»· lá»‡ thÃ nh cÃ´ng tá»›i 99.7%. Vá»›i thuáº­t toÃ¡n DQN, tá»· lá»‡ háº¡ cÃ¡nh thÃ nh cÃ´ng cá»§a tÃ u vÅ© trá»¥ Ä‘áº¡t má»©c khÃ¡ cao vá»›i 95.6%, trong khi Double-DQN â€“ phiÃªn báº£n cáº£i tiáº¿n cá»§a DQN â€“ Ä‘áº¡t tá»· lá»‡ thÃ nh cÃ´ng lÃªn tá»›i 99.7%. Äiá»u nÃ y cho tháº¥y ráº±ng cáº£ hai thuáº­t toÃ¡n Ä‘á»u cÃ³ kháº£ nÄƒng há»c chÃ­nh sÃ¡ch hiá»‡u quáº£ trong mÃ´i trÆ°á»ng LunarLander-v3. Tuy nhiÃªn, Double DQN vÆ°á»£t trá»™i hÆ¡n nhá» kháº£ nÄƒng giáº£m thiá»ƒu hiá»‡n tÆ°á»£ng quÃ¡ Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹ Q (Q-value overestimation), vá»‘n lÃ  má»™t Ä‘iá»ƒm yáº¿u cá»‘ há»¯u cá»§a DQN thuáº§n tÃºy.Double DQN khÃ´ng chá»‰ cáº£i thiá»‡n hiá»‡u suáº¥t mÃ  cÃ²n Ä‘áº£m báº£o sá»± á»•n Ä‘á»‹nh lÃ¢u dÃ i cho quÃ¡ trÃ¬nh há»c trong mÃ´i trÆ°á»ng giáº£ láº­p háº¡ cÃ¡nh nÃ y. NÃ³ lÃ  lá»±a chá»n Æ°u tiÃªn khi triá»ƒn khai cÃ¡c bÃ i toÃ¡n cÃ³ khÃ´ng gian hÃ nh Ä‘á»™ng rá»i ráº¡c vÃ  yÃªu cáº§u há»c chÃ­nh sÃ¡ch tá»‘i Æ°u á»•n Ä‘á»‹nh Ä‘áº·c biá»‡t vá»›i mÃ´i trÆ°á»ng Lunar-Lander-v3.  

## ğŸ“½ï¸ Video mÃ´ phá»ng mÃ´ hÃ¬nh 
- ğŸ¬ **Ná»™i dung**: Video mÃ´ táº£ quÃ¡ trÃ¬nh háº¡ cÃ¡nh thÃ nh cÃ´ng cá»§a tÃ u vÅ© trá»¥. 
- ğŸ“º **Xem video**:  
ğŸ‘‰ [ğŸ“½ï¸ Nháº¥n Ä‘á»ƒ xem video](lunar_lander_video.mp4)

---

## ğŸ”® Cáº£i tiáº¿n trong tÆ°Æ¡ng lai

DÃ¹ mÃ´ hÃ¬nh Double DQN Ä‘Ã£ Ä‘áº¡t káº¿t quáº£ ráº¥t kháº£ quan vá»›i mÃ´i trÆ°á»ng LunarLander-v3, váº«n cÃ²n nhiá»u hÆ°á»›ng phÃ¡t triá»ƒn Ä‘á»ƒ nÃ¢ng cao hiá»‡u suáº¥t, Ä‘á»™ á»•n Ä‘á»‹nh vÃ  kháº£ nÄƒng má»Ÿ rá»™ng:

### ğŸš€ 1. Sá»­ dá»¥ng thuáº­t toÃ¡n nÃ¢ng cao hÆ¡n
- âœ… **Dueling DQN**: TÃ¡ch biá»‡t giÃ¡ trá»‹ tráº¡ng thÃ¡i vÃ  giÃ¡ trá»‹ hÃ nh Ä‘á»™ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ hÆ¡n.
- âœ… **Prioritized Experience Replay**: Æ¯u tiÃªn cÃ¡c tráº£i nghiá»‡m cÃ³ áº£nh hÆ°á»Ÿng lá»›n Ä‘áº¿n quÃ¡ trÃ¬nh há»c.
- âœ… **Noisy Nets**: ThÃªm nhiá»…u vÃ o máº¡ng Q Ä‘á»ƒ cáº£i thiá»‡n chiáº¿n lÆ°á»£c thÄƒm dÃ² (exploration).

### ğŸ§  2. Cáº£i thiá»‡n kiáº¿n trÃºc máº¡ng há»c sÃ¢u
- ğŸ“ˆ TÄƒng sá»‘ lá»›p hoáº·c Ä‘Æ¡n vá»‹ áº©n Ä‘á»ƒ há»c cÃ¡c biá»ƒu diá»…n phá»©c táº¡p hÆ¡n.
- ğŸ§ª Thá»­ nghiá»‡m vá»›i cÃ¡c activation function khÃ¡c nhÆ° Swish, Mish thay cho ReLU.

### ğŸ§® 3. Há»— trá»£ huáº¥n luyá»‡n song song hoáº·c phÃ¢n tÃ¡n
- ğŸ’» Táº­n dá»¥ng nhiá»u GPU hoáº·c multi-threading Ä‘á»ƒ tÄƒng tá»‘c huáº¥n luyá»‡n.
- â˜ï¸ Káº¿t há»£p vá»›i ná»n táº£ng nhÆ° Ray RLlib hoáº·c cÃ¡c thÆ° viá»‡n phÃ¢n tÃ¡n.

### ğŸ¯ 4. Má»Ÿ rá»™ng sang mÃ´i trÆ°á»ng phá»©c táº¡p hÆ¡n
- ğŸŒ Ãp dá»¥ng vá»›i cÃ¡c mÃ´i trÆ°á»ng khÃ¡c nhÆ°:
  - `BipedalWalker-v3`
  - `CarRacing-v2`
  - MÃ´i trÆ°á»ng 3D hoáº·c Ä‘iá»u khiá»ƒn robot thá»±c táº¿ (sim2real)

### ğŸ›  5. Cáº£i thiá»‡n pháº§n hiá»ƒn thá»‹ vÃ  phÃ¢n tÃ­ch
- ğŸ“Š Giao diá»‡n trá»±c quan hÃ³a cÃ¡c bÆ°á»›c huáº¥n luyá»‡n (tensorboard hoáº·c matplotlib nÃ¢ng cao).
- ğŸ¥ Tá»± Ä‘á»™ng ghi láº¡i quÃ¡ trÃ¬nh huáº¥n luyá»‡n thÃ nh video `.mp4` sau má»—i n episode.
- ğŸ“ˆ Biá»ƒu Ä‘á»“ so sÃ¡nh DQN vs Double DQN theo reward, tá»‘c Ä‘á»™ há»™i tá»¥, v.v.

---
## ğŸ“š TÃ i liá»‡u tham kháº£o

DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ tÃ i liá»‡u vÃ  nguá»“n há»c thuáº­t uy tÃ­n Ä‘Ã£ Ä‘Æ°á»£c tham kháº£o trong quÃ¡ trÃ¬nh phÃ¡t triá»ƒn dá»± Ã¡n:

### ğŸ“„ BÃ i bÃ¡o khoa há»c & lÃ½ thuyáº¿t

- ğŸ“˜ **Mnih et al. (2015)** â€” *"Human-level control through deep reinforcement learning"*  
  [https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236)

- ğŸ“˜ **Van Hasselt et al. (2016)** â€” *"Deep Reinforcement Learning with Double Q-learning"*  
  [https://arxiv.org/abs/1509.06461](https://arxiv.org/abs/1509.06461)

- ğŸ“˜ **Schaul et al. (2016)** â€” *"Prioritized Experience Replay"*  
  [https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952)

### ğŸ“š TÃ i liá»‡u ká»¹ thuáº­t & thÆ° viá»‡n

- ğŸ§  **OpenAI Gymnasium Documentation**  
  [https://gymnasium.farama.org](https://gymnasium.farama.org)

- ğŸ”§ **PyTorch Official Docs**  
  [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)

- ğŸ“ˆ **Matplotlib for Animation**  
  [https://matplotlib.org/stable/api/animation_api.html](https://matplotlib.org/stable/api/animation_api.html)

### ğŸ¥ Há»c thÃ´ng qua thá»±c hÃ nh & nguá»“n má»Ÿ

- ğŸ“º **Deep Reinforcement Learning Nanodegree (Udacity)**  
  [https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)

- ğŸ’» **GitHub: DQN & RL Repositories (chá»n lá»c)**  
  - https://github.com/higgsfield/RL-Adventure
  - https://github.com/dennybritz/reinforcement-learning

---

> ğŸ” *Táº¥t cáº£ tÃ i liá»‡u trÃªn Ä‘Ã£ Ä‘Æ°á»£c chá»n lá»c nháº±m Ä‘áº£m báº£o Ä‘á»™ tin cáº­y vÃ  há»— trá»£ tá»‘i Æ°u cho quÃ¡ trÃ¬nh triá»ƒn khai vÃ  cáº£i tiáº¿n mÃ´ hÃ¬nh.*
